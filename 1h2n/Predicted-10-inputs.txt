#this exmple it's describe the gradient-descent for 10 inputs
import numpy as np

def sigmoid(sop):
    return 1.0/(1+np.exp(-1*sop))

def error(predicted, target):
    return np.power(predicted-target, 2)

def error_predicted_deriv(predicted, target):
    return 2*(predicted-target)

def predictive_sop_deriv(sop):
    return sigmoid(sop)*(1.0-sigmoid(sop))

def sop_w_deriv(x):
    return x

def update_w(w, grad, learning_rate):
    return w - learning_rate*grad

x = np.array([0.1, 0.4, 1.1, 1.3, 1.8, 2.0, 0.01, 0.9, 0.8, 1.6])
target = np.array([0.2])

learning_rate = 0.1
#every time we initialize the w with random.rand
w = np.random.rand(10)
print("weight = ",w)

for k in range(1000):
    #Forward Pass
    y = np.sum(w*x)
    predicted = sigmoid(y)
    err = error(predicted,target)
    print(f"the error value is: {err}")
    g1 = error_predicted_deriv(predicted,target)
    g2 = predictive_sop_deriv(y)
    g3 = sop_w_deriv(x)

    grad = g3 * g2 * g1
    w = update_w(w, grad, learning_rate)
    print(f"the predicted value is: {predicted}")

